---
title: "Lab Exercise - Web Scraping"
author: "Luis Correia - Student No. 1006508566"
date: "February 5th 2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
  rmarkdown::pdf_document:
    fig_caption: yes
    number_sections: yes
header-includes:
- \usepackage[margin=1in]{geometry} 
- \usepackage{amsmath,amsthm,amssymb,amsfonts}
- \usepackage{relsize}
- \usepackage{enumerate}
- \usepackage{setspace}
- \usepackage{tikz}
- \usepackage{bm}
- \usepackage[utf8]{inputenc}
- \usepackage{mathtools, nccmath}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[CO,CE]{---STA2201-Applied Statistics II---}
- \fancyfoot[C]{Luis Correia - Student No. 1006508566}
- \fancyfoot[RO, LE] {\thepage}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

Today we will be extracting some useful data from websites. There's a bunch of different ways to web-scrape, but we'll be exploring using the `rvest` package in R, that helps you to deal with parsing html. 

Why is web scraping useful? If our research involves getting data from a website that isn't already in a easily downloadable form, it improves the reproducibility of our research. Once you get a scraper working, it's less prone to human error than copy-pasting, for example, and much easier for someone else to see what you did. 

## A note on responsibility

Seven principles for web-scraping responsibly:

1. Try to use an API.
2. Check robots.txt. (e.g. https://www.utoronto.ca/robots.txt)
3. Slow down (why not only visit the website once a minute if you can just run your data collection in the background while you're doing other things?).
4. Consider the timing (if it's a retailer then why not set your script to run overnight?).
5. Only scrape once (save the data as you go and monitor where you are up to).
6. Don't republish the data you scraped (cf datasets that create based off it).
7. Take ownership (add contact details to your scripts, don't hide behind VPNs, etc)

# Extracting data on opioid prescriptions from CDC

In Assignment 1 the `opioids` dataset contained data by state and year on the opioid prescription rate. I grabbed this data from the [CDC website](https://www.cdc.gov/drugoverdose/maps/rxrate-maps.html). While the data are nicely presented and mapped, there's no nice way of downloading the data for each year as a csv or similar form. So let's use `rvest` to extract the data. We'll also load in `janitor` to clean up column names etc later on. 

```{r}
library(tidyverse)
library(rvest)
library(janitor)
```

## Getting the data for 2008

Have a look at the website at the url below. It shows a map and (if you scroll down) a table of state prescription rates in 2008. Let's read in the html of this page. 

```{r}
cdcpage <- "https://www.cdc.gov/drugoverdose/maps/rxstate2008.html"
cdc <- read_html(cdcpage)
cdc
```
Note that it has two main parts, a head and body. For the majority of use cases, you will probably be interested in the body. You can select a node using `html_node()` and then see its child nodes using `html_children()`.

```{r}
body_nodes <- cdc %>% 
 html_node("body") %>% 
 html_children()
body_nodes
```

We can keep going down to see the nodes within the nodes, just by piping again:

```{r}
body_nodes[[7]] %>% 
  html_children() %>%
  html_children() %>% `[[`(3) # pull the third element
```

## Inspecting elements of a website

The above is still fairly impenetrable. But we can get hints from the website itself. Using Chrome (or Firefox) you can highlight a part of the website of interest (say, 'Alabama'), right click and choose 'Inspect'. That gives you info on the underlying html of the webpage on the right hand side. Alternatively, and probably easier to find what we want, right click on the webpage and choose View Page Source. This opens a new window with all the html. Do a search for the world 'Alabama'. Now we can see the code for the table. We can see that the data we want are all within `tr`. So let's extract those nodes:

```{r}
cdc %>% 
  html_nodes("tr") 
```

Great, now we're getting somewhere. We only want the text, not the html rubbish, so let's extract that:

```{r}
table_text <- cdc %>% 
  html_nodes("tr") %>% 
  html_text() 

table_text
```

This is almost useful! Turning it into a tibble and using `separate` to get the variables into separate columns gets us almost there:

```{r}
rough_table <- table_text %>% 
  as_tibble() %>% 
  separate(value, into = c("state", "abbrev", "rate"), sep = "\n", extra = "drop") 
rough_table
```

Now we can just divert to our standard tidyverse cleaning skills (`janitor` functions help here) to tidy it up:

```{r}
d_prescriptions <- rough_table %>% 
  janitor::row_to_names(1) %>% 
  janitor::clean_names() %>% 
  rename(prescribing_rate = x2008_prescribing_rate) %>% 
  mutate(prescribing_rate = as.numeric(prescribing_rate)) 

d_prescriptions
```

Now we have clean data for 2008! Great success. 

## Take-aways

This example showed you how to extract a particular table from a particular website. The take-away is to inspect the page html, find where what you want is hiding, and then use the tools in `rvest` (`html_nodes()` and `html_text()` particularly useful) to extract it. 

## Question 1

Add a year column to `d_prescriptions`.

```{r}
d_prescriptions <- d_prescriptions %>% 
  mutate(year = 2008)
d_prescriptions
```


## Getting all the other years

Now I want you to get data for 2009-2016 and save it into one big tibble. If you go to https://www.cdc.gov/drugoverdose/maps/rxrate-maps.html, on the right hand side there's hyperlinks to all the years under "U.S. State Prescribing Rate Maps". 

Click on 2009. Look at the url. Confirm that it's exactly the same format as the url for 2008, except the year has changed. This is useful, because we can just loop through in an automated way, changing the year as we go. 

## Question 2

Make a vector of the urls for each year, storing them as strings. Here's some code to fill in the gaps (remember to remove eval = F):

```{r}
# https://www.cdc.gov/drugoverdose/maps/rxcounty2009.html  - Base URL for 2009
years <- c(2009:2016)
base_url <- "https://www.cdc.gov/drugoverdose/maps/rxcounty"
year_urls <- paste0(base_url,years[1:8],".html")
year_urls
```

## Question 3

By filling in the code below, extract the prescriptions data for the years 2008-2016, and store in the one tibble. Make sure you have a column for state, state abbreviation, prescription rate and year. (remember to remove eval = F)

Note: if you copy paste the code above and put it in the loop, you will get an error because the prescriptions data column name has the year in it. You can get around this however you want, but you can define column names based on a variable making use of `!!` e.g. `!!paste0("x",years[i],"_prescribing_rate")`

Another note: notice the last year is 2016, not 2017. If you look at the 2017 page, you'll notice the format of the column names has changed. So you would have to write some more code to deal with this special case. You don't have to do this for the lab, but if you want extra practice, maybe this would be a good exercise. 

```{r}
prescriptions_all_years <- c()

for(i in 1:length(years)){
  cdc <- read_html(year_urls[i])
  body_nodes <- cdc %>% 
    html_node("body") %>% 
    html_children()
  table_text <- cdc %>% 
    html_nodes("tr") %>% 
    html_text() 
  rough_table <- table_text %>% 
    as_tibble() %>% 
    separate(value, into = c("county", "state", "abbrev", "rate"), sep = "\n", extra = "drop") 
  d_prescriptions <- rough_table %>% 
    janitor::row_to_names(1) %>% 
    janitor::clean_names() %>% 
    rename(prescribing_rate = paste0("x",years[i],"_prescribing_rate")) %>% 
    mutate(prescribing_rate = as.numeric(prescribing_rate), year = years[i]) 
  
  prescriptions_all_years <- bind_rows(prescriptions_all_years, d_prescriptions) # assuming your tibble is called d_prescriptions
  Sys.sleep(1) # wait a sec until going again
}
```

Print the head and tail of your dataset (remember to remove eval = F)

```{r}
head(prescriptions_all_years)
tail(prescriptions_all_years)
```


# Question 4: Install rstan and brms

We will be using the packages `rstan` and `brms` from next week. Please install these. Here's some instructions:

- https://github.com/paul-buerkner/brms
- https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started

In most cases it will be straightforward and may not need much more than `install.packages()`, but particularly if you have Catalina, you might run into issues. 

To make sure it works, run the following code:

```{r}
library(brms)

x <- rnorm(100)
y <- 1 + 2*x + rnorm(100)
d <- tibble(x = x, y= y)

mod <- brm(y~x, data = d)
summary(mod)

```

